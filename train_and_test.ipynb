{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "TPgogXk6c_NA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "gfUD4gwpdGtD"
   },
   "outputs": [],
   "source": [
    "f=open('dataset.npz','rb')\n",
    "npzfile = np.load(f)\n",
    "images=npzfile['arr_0']\n",
    "labels=npzfile['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3880,
     "status": "ok",
     "timestamp": 1531924230975,
     "user": {
      "displayName": "Hari Krishnan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116369642993621286939"
     },
     "user_tz": -330
    },
    "id": "r5QfrD5fdUWN",
    "outputId": "43a35426-c78f-4e9c-ed02-148bacc55e9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22292, 64, 64, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and testing set\n",
    "split_point = int(len(images)*0.9)\n",
    "test_img=list(images[:-split_point])\n",
    "test_labels=list(labels[:-split_point])\n",
    "train_img=list(images[:split_point])\n",
    "train_labels=list(labels[:split_point])\n",
    "train_img=np.array(train_img)\n",
    "test_img=np.array(test_img)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "iEq3oTE3ekAP"
   },
   "outputs": [],
   "source": [
    "# Initializing learning parameters\n",
    "lr = 0.0002\n",
    "epoch = 100\n",
    "batch_size = 10\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "PN71nOihenRX"
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "x = tf.placeholder('float',[None,64,64,1])\n",
    "y_ = tf.placeholder('float',[None,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "U_UuF72XerR5"
   },
   "outputs": [],
   "source": [
    "end = 0\n",
    "def next_training_batch(batch_size):\n",
    "    global end\n",
    "    start = end\n",
    "    end += batch_size\n",
    "    if end > len(train_img):\n",
    "        difference = batch_size - (end - len(train_img))\n",
    "        return_img = np.concatenate((train_img[end-batch_size:],train_img[:difference]))\n",
    "        return_labels=np.concatenate((train_labels[end-batch_size:],train_labels[:difference]))\n",
    "        end = difference\n",
    "    else:\n",
    "        return_img=train_img[start:end]\n",
    "        return_labels = train_labels[start:end]\n",
    "    return (return_img,return_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1413,
     "status": "ok",
     "timestamp": 1531924370255,
     "user": {
      "displayName": "Hari Krishnan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116369642993621286939"
     },
     "user_tz": -330
    },
    "id": "QNb9CYiretxp",
    "outputId": "cae50760-e0c6-40ed-f461-7b29e91d8cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64, 64, 32)\n",
      "(?, 32, 32, 32)\n",
      "(?, 32, 32, 64)\n",
      "(?, 16, 16, 64)\n",
      "(?, 16, 16, 128)\n",
      "(?, 8, 8, 128)\n",
      "(?, 8192)\n",
      "(?, 1024)\n"
     ]
    }
   ],
   "source": [
    "# First Layer\n",
    "W_conv1 = weight_variable([2, 2, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,64,64,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "print(h_conv1.shape)\n",
    "print(h_pool1.shape)\n",
    "# Second Layer\n",
    "W_conv2 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "W_conv2_2 = weight_variable([3, 3, 64, 64])\n",
    "b_conv2_2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_conv2_2 = tf.nn.relu(conv2d(h_conv2, W_conv2_2) + b_conv2_2)\n",
    "h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "print(h_conv2.shape)\n",
    "print(h_pool2.shape)\n",
    "#Third Layer \n",
    "W_conv3 = weight_variable([3, 3, 64, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "print(h_conv3.shape)\n",
    "print(h_pool3.shape)\n",
    "# Fourth Layer\n",
    "W_conv4 = weight_variable([5, 5, 128, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "'''h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "h_pool4 = max_pool_2x2(h_conv4)\n",
    "print(h_conv4.shape)\n",
    "print(h_pool4.shape)\n",
    "#Fifth Layer\n",
    "W_conv5 = weight_variable([2, 2, 64, 32])\n",
    "b_conv5 = bias_variable([32])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_pool4, W_conv5) + b_conv5)\n",
    "h_pool5 = max_pool_2x2(h_conv5)\n",
    "print(h_conv5.shape)\n",
    "print(h_pool5.shape)'''\n",
    "# Densely Connected Layer\n",
    "W_fc1 = weight_variable([8 * 8 * 128, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool3, [-1, 8*8*128])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "print(h_pool2_flat.shape)\n",
    "print(h_fc1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1179,
     "status": "ok",
     "timestamp": 1531924373938,
     "user": {
      "displayName": "Hari Krishnan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116369642993621286939"
     },
     "user_tz": -330
    },
    "id": "EvySRpquewnE",
    "outputId": "e51d5b0b-c0f5-404d-e496-f8803b5a7e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1024)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "print(h_fc1_drop.shape)\n",
    "# Readout Layer\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "print(y_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3e3YvEEtezL2"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5457
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5366205,
     "status": "ok",
     "timestamp": 1531929747313,
     "user": {
      "displayName": "Hari Krishnan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116369642993621286939"
     },
     "user_tz": -330
    },
    "id": "kUXebZVle2PB",
    "outputId": "5643125b-bf4c-4928-ef5a-dd5991729d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.46\n",
      "step 500, training accuracy 0.74\n",
      "step 1000, training accuracy 0.68\n",
      "step 1500, training accuracy 0.6\n",
      "step 2000, training accuracy 0.62\n",
      "step 2500, training accuracy 0.6\n",
      "step 3000, training accuracy 0.74\n",
      "step 3500, training accuracy 0.68\n",
      "step 4000, training accuracy 0.74\n",
      "step 4500, training accuracy 0.68\n",
      "step 5000, training accuracy 0.84\n",
      "step 5500, training accuracy 0.74\n",
      "step 6000, training accuracy 0.74\n",
      "step 6500, training accuracy 0.78\n",
      "step 7000, training accuracy 0.78\n",
      "step 7500, training accuracy 0.84\n",
      "step 8000, training accuracy 0.68\n",
      "step 8500, training accuracy 0.86\n",
      "step 9000, training accuracy 0.86\n",
      "step 9500, training accuracy 0.94\n",
      "step 10000, training accuracy 0.96\n",
      "step 10500, training accuracy 0.92\n",
      "step 11000, training accuracy 0.9\n",
      "step 11500, training accuracy 0.96\n",
      "step 12000, training accuracy 0.94\n",
      "step 12500, training accuracy 0.94\n",
      "step 13000, training accuracy 0.96\n",
      "step 13500, training accuracy 0.96\n",
      "step 14000, training accuracy 1\n",
      "step 14500, training accuracy 0.94\n",
      "step 15000, training accuracy 0.96\n",
      "step 15500, training accuracy 0.98\n",
      "step 16000, training accuracy 1\n",
      "step 16500, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17500, training accuracy 0.98\n",
      "step 18000, training accuracy 1\n",
      "step 18500, training accuracy 1\n",
      "step 19000, training accuracy 0.98\n",
      "step 19500, training accuracy 1\n",
      "step 20000, training accuracy 1\n",
      "step 20500, training accuracy 1\n",
      "step 21000, training accuracy 1\n",
      "step 21500, training accuracy 1\n",
      "step 22000, training accuracy 1\n",
      "step 22500, training accuracy 1\n",
      "step 23000, training accuracy 1\n",
      "step 23500, training accuracy 1\n",
      "step 24000, training accuracy 1\n",
      "step 24500, training accuracy 1\n",
      "step 25000, training accuracy 1\n",
      "step 25500, training accuracy 1\n",
      "step 26000, training accuracy 1\n",
      "step 26500, training accuracy 1\n",
      "step 27000, training accuracy 1\n",
      "step 27500, training accuracy 1\n",
      "step 28000, training accuracy 1\n",
      "step 28500, training accuracy 1\n",
      "step 29000, training accuracy 1\n",
      "step 29500, training accuracy 1\n",
      "step 30000, training accuracy 1\n",
      "step 30500, training accuracy 1\n",
      "step 31000, training accuracy 1\n",
      "step 31500, training accuracy 1\n",
      "step 32000, training accuracy 1\n",
      "step 32500, training accuracy 1\n",
      "step 33000, training accuracy 1\n",
      "step 33500, training accuracy 1\n",
      "step 34000, training accuracy 1\n",
      "step 34500, training accuracy 1\n",
      "step 35000, training accuracy 1\n",
      "step 35500, training accuracy 1\n",
      "step 36000, training accuracy 1\n",
      "step 36500, training accuracy 1\n",
      "step 37000, training accuracy 1\n",
      "step 37500, training accuracy 1\n",
      "step 38000, training accuracy 1\n",
      "step 38500, training accuracy 1\n",
      "step 39000, training accuracy 1\n",
      "step 39500, training accuracy 1\n",
      "step 40000, training accuracy 1\n",
      "step 40500, training accuracy 1\n",
      "step 41000, training accuracy 1\n",
      "step 41500, training accuracy 1\n",
      "step 42000, training accuracy 1\n",
      "step 42500, training accuracy 1\n",
      "step 43000, training accuracy 1\n",
      "step 43500, training accuracy 1\n",
      "step 44000, training accuracy 1\n",
      "step 44500, training accuracy 1\n",
      "step 45000, training accuracy 1\n",
      "step 45500, training accuracy 1\n",
      "step 46000, training accuracy 1\n",
      "step 46500, training accuracy 1\n",
      "step 47000, training accuracy 1\n",
      "step 47500, training accuracy 1\n",
      "step 48000, training accuracy 1\n",
      "step 48500, training accuracy 1\n",
      "step 49000, training accuracy 1\n",
      "step 49500, training accuracy 1\n",
      "step 50000, training accuracy 1\n",
      "step 50500, training accuracy 1\n",
      "step 51000, training accuracy 1\n",
      "step 51500, training accuracy 1\n",
      "step 52000, training accuracy 1\n",
      "step 52500, training accuracy 1\n",
      "step 53000, training accuracy 1\n",
      "step 53500, training accuracy 1\n",
      "step 54000, training accuracy 1\n",
      "step 54500, training accuracy 1\n",
      "step 55000, training accuracy 1\n",
      "step 55500, training accuracy 1\n",
      "step 56000, training accuracy 1\n",
      "step 56500, training accuracy 1\n",
      "step 57000, training accuracy 1\n",
      "step 57500, training accuracy 1\n",
      "step 58000, training accuracy 1\n",
      "step 58500, training accuracy 1\n",
      "step 59000, training accuracy 1\n",
      "step 59500, training accuracy 1\n",
      "step 60000, training accuracy 1\n",
      "step 60500, training accuracy 1\n",
      "step 61000, training accuracy 1\n",
      "step 61500, training accuracy 1\n",
      "step 62000, training accuracy 0.98\n",
      "step 62500, training accuracy 1\n",
      "step 63000, training accuracy 1\n",
      "step 63500, training accuracy 1\n",
      "step 64000, training accuracy 1\n",
      "step 64500, training accuracy 1\n",
      "step 65000, training accuracy 1\n",
      "step 65500, training accuracy 1\n",
      "step 66000, training accuracy 1\n",
      "step 66500, training accuracy 1\n",
      "step 67000, training accuracy 1\n",
      "step 67500, training accuracy 1\n",
      "step 68000, training accuracy 1\n",
      "step 68500, training accuracy 1\n",
      "step 69000, training accuracy 1\n",
      "step 69500, training accuracy 1\n",
      "step 70000, training accuracy 1\n",
      "step 70500, training accuracy 1\n",
      "step 71000, training accuracy 1\n",
      "step 71500, training accuracy 1\n",
      "step 72000, training accuracy 1\n",
      "step 72500, training accuracy 1\n",
      "step 73000, training accuracy 1\n",
      "step 73500, training accuracy 1\n",
      "step 74000, training accuracy 1\n",
      "step 74500, training accuracy 1\n",
      "step 75000, training accuracy 1\n",
      "step 75500, training accuracy 1\n",
      "step 76000, training accuracy 1\n",
      "step 76500, training accuracy 1\n",
      "step 77000, training accuracy 1\n",
      "step 77500, training accuracy 1\n",
      "step 78000, training accuracy 1\n",
      "step 78500, training accuracy 1\n",
      "step 79000, training accuracy 1\n",
      "step 79500, training accuracy 1\n",
      "step 80000, training accuracy 1\n",
      "step 80500, training accuracy 1\n",
      "step 81000, training accuracy 1\n",
      "step 81500, training accuracy 1\n",
      "step 82000, training accuracy 1\n",
      "step 82500, training accuracy 1\n",
      "step 83000, training accuracy 1\n",
      "step 83500, training accuracy 1\n",
      "step 84000, training accuracy 1\n",
      "step 84500, training accuracy 1\n",
      "step 85000, training accuracy 1\n",
      "step 85500, training accuracy 1\n",
      "step 86000, training accuracy 1\n",
      "step 86500, training accuracy 1\n",
      "step 87000, training accuracy 1\n",
      "step 87500, training accuracy 1\n",
      "step 88000, training accuracy 1\n",
      "step 88500, training accuracy 1\n",
      "step 89000, training accuracy 1\n",
      "step 89500, training accuracy 1\n",
      "step 90000, training accuracy 1\n",
      "step 90500, training accuracy 1\n",
      "step 91000, training accuracy 1\n",
      "step 91500, training accuracy 1\n",
      "step 92000, training accuracy 1\n",
      "step 92500, training accuracy 1\n",
      "step 93000, training accuracy 1\n",
      "step 93500, training accuracy 1\n",
      "step 94000, training accuracy 1\n",
      "step 94500, training accuracy 1\n",
      "step 95000, training accuracy 1\n",
      "step 95500, training accuracy 1\n",
      "step 96000, training accuracy 1\n",
      "step 96500, training accuracy 1\n",
      "step 97000, training accuracy 1\n",
      "step 97500, training accuracy 1\n",
      "step 98000, training accuracy 1\n",
      "step 98500, training accuracy 1\n",
      "step 99000, training accuracy 1\n",
      "step 99500, training accuracy 1\n",
      "step 100000, training accuracy 1\n",
      "step 100500, training accuracy 1\n",
      "step 101000, training accuracy 1\n",
      "step 101500, training accuracy 1\n",
      "step 102000, training accuracy 1\n",
      "step 102500, training accuracy 1\n",
      "step 103000, training accuracy 1\n",
      "step 103500, training accuracy 1\n",
      "step 104000, training accuracy 1\n",
      "step 104500, training accuracy 1\n",
      "step 105000, training accuracy 1\n",
      "step 105500, training accuracy 1\n",
      "step 106000, training accuracy 1\n",
      "step 106500, training accuracy 1\n",
      "step 107000, training accuracy 1\n",
      "step 107500, training accuracy 1\n",
      "step 108000, training accuracy 1\n",
      "step 108500, training accuracy 1\n",
      "step 109000, training accuracy 1\n",
      "step 109500, training accuracy 1\n",
      "step 110000, training accuracy 1\n",
      "step 110500, training accuracy 1\n",
      "step 111000, training accuracy 1\n",
      "step 111500, training accuracy 1\n",
      "step 112000, training accuracy 1\n",
      "step 112500, training accuracy 1\n",
      "step 113000, training accuracy 1\n",
      "step 113500, training accuracy 1\n",
      "step 114000, training accuracy 1\n",
      "step 114500, training accuracy 1\n",
      "step 115000, training accuracy 1\n",
      "step 115500, training accuracy 1\n",
      "step 116000, training accuracy 1\n",
      "step 116500, training accuracy 1\n",
      "step 117000, training accuracy 1\n",
      "step 117500, training accuracy 1\n",
      "step 118000, training accuracy 1\n",
      "step 118500, training accuracy 1\n",
      "step 119000, training accuracy 1\n",
      "step 119500, training accuracy 1\n",
      "step 120000, training accuracy 1\n",
      "step 120500, training accuracy 1\n",
      "step 121000, training accuracy 1\n",
      "step 121500, training accuracy 1\n",
      "step 122000, training accuracy 1\n",
      "step 122500, training accuracy 1\n",
      "step 123000, training accuracy 1\n",
      "step 123500, training accuracy 1\n",
      "step 124000, training accuracy 1\n",
      "step 124500, training accuracy 1\n",
      "step 125000, training accuracy 1\n",
      "step 125500, training accuracy 1\n",
      "step 126000, training accuracy 1\n",
      "step 126500, training accuracy 1\n",
      "step 127000, training accuracy 1\n",
      "step 127500, training accuracy 1\n",
      "step 128000, training accuracy 1\n",
      "step 128500, training accuracy 1\n",
      "step 129000, training accuracy 1\n",
      "step 129500, training accuracy 1\n",
      "step 130000, training accuracy 1\n",
      "step 130500, training accuracy 1\n",
      "step 131000, training accuracy 1\n",
      "step 131500, training accuracy 1\n",
      "step 132000, training accuracy 1\n",
      "step 132500, training accuracy 1\n",
      "step 133000, training accuracy 1\n",
      "step 133500, training accuracy 1\n",
      "step 134000, training accuracy 1\n",
      "step 134500, training accuracy 1\n",
      "step 135000, training accuracy 1\n",
      "step 135500, training accuracy 1\n",
      "step 136000, training accuracy 1\n",
      "step 136500, training accuracy 1\n",
      "step 137000, training accuracy 1\n",
      "step 137500, training accuracy 1\n",
      "step 138000, training accuracy 1\n",
      "step 138500, training accuracy 1\n",
      "step 139000, training accuracy 1\n",
      "step 139500, training accuracy 1\n",
      "step 140000, training accuracy 1\n",
      "step 140500, training accuracy 1\n",
      "step 141000, training accuracy 1\n",
      "step 141500, training accuracy 1\n",
      "step 142000, training accuracy 1\n",
      "step 142500, training accuracy 1\n",
      "step 143000, training accuracy 1\n",
      "step 143500, training accuracy 1\n",
      "step 144000, training accuracy 1\n",
      "step 144500, training accuracy 1\n",
      "step 145000, training accuracy 1\n",
      "step 145500, training accuracy 1\n",
      "step 146000, training accuracy 1\n",
      "step 146500, training accuracy 1\n",
      "step 147000, training accuracy 1\n",
      "step 147500, training accuracy 1\n",
      "step 148000, training accuracy 1\n",
      "step 148500, training accuracy 1\n",
      "step 149000, training accuracy 1\n",
      "step 149500, training accuracy 1\n",
      "step 150000, training accuracy 1\n",
      "step 150500, training accuracy 1\n",
      "step 151000, training accuracy 1\n",
      "step 151500, training accuracy 1\n",
      "step 152000, training accuracy 1\n",
      "step 152500, training accuracy 1\n",
      "step 153000, training accuracy 1\n",
      "step 153500, training accuracy 1\n",
      "step 154000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 154500, training accuracy 1\n",
      "step 155000, training accuracy 1\n",
      "step 155500, training accuracy 1\n",
      "step 156000, training accuracy 1\n",
      "step 156500, training accuracy 1\n",
      "step 157000, training accuracy 1\n",
      "step 157500, training accuracy 1\n",
      "step 158000, training accuracy 1\n",
      "step 158500, training accuracy 1\n",
      "step 159000, training accuracy 1\n",
      "step 159500, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(160000):\n",
    "    batch = next_training_batch(50)\n",
    "    if i%500 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4442,
     "status": "ok",
     "timestamp": 1531929751880,
     "user": {
      "displayName": "Hari Krishnan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116369642993621286939"
     },
     "user_tz": -330
    },
    "id": "7UpIfCWae48R",
    "outputId": "bca2989e-e6af-4943-f280-5de7382d8d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.999596\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: test_img, y_: test_labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "BypjTXa4opxj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Cat V Dog.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
